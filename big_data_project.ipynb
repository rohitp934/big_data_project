{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b60d597",
   "metadata": {},
   "source": [
    "## Initialization and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8b8b3b5ea8b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0212b0ccc479a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "from dask.distributed import Client\n",
    "import dask.distributed\n",
    "import dask.bag as db\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.api_core import retry\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# Start a Dask client for parallel computation\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c8bd1f43f3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!earthengine authenticate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b3da35b07d8fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32a7087cf6c1edf6",
   "metadata": {},
   "source": [
    "## Load data acquisition and EDA step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f957863e0f113",
   "metadata": {},
   "source": [
    "# Change bigdata-ahhcash to your project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa2b5af3597a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize(project='bigdata-ahhcash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73548505a6f45574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_image() -> ee.Image:\n",
    "    # Remap the ESA classifications into the Dynamic World classifications\n",
    "    fromValues = [10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 100]\n",
    "    toValues = [1, 5, 2, 4, 6, 7, 8, 0, 3, 3, 7]\n",
    "    return (\n",
    "        ee.Image(\"ESA/WorldCover/v100/2020\")\n",
    "        .select(\"Map\")\n",
    "        .remap(fromValues, toValues)\n",
    "        .rename(\"landcover\")\n",
    "        .unmask(0)  # fill missing values with 0 (water)\n",
    "        .byte()  # 9 classifications fit into an unsinged 8-bit integer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed8e0b2cc45f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "land_cover_image = get_label_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8a16fca4af6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified polygons covering most land areas in the world.\n",
    "WORLD_POLYGONS = [\n",
    "    # Americas\n",
    "    [(-33.0, -7.0), (-55.0, 53.0), (-166.0, 65.0), (-68.0, -56.0)],\n",
    "    # Africa, Asia, Europe\n",
    "    [\n",
    "        (74.0, 71.0),\n",
    "        (166.0, 55.0),\n",
    "        (115.0, -11.0),\n",
    "        (74.0, -4.0),\n",
    "        (20.0, -38.0),\n",
    "        (-29.0, 25.0),\n",
    "    ],\n",
    "    # Australia\n",
    "    [(170.0, -47.0), (179.0, -37.0), (167.0, -12.0), (128.0, 17.0), (106.0, -29.0)],\n",
    "]\n",
    "\n",
    "# Function to get stratified sample points\n",
    "def get_stratified_sample_coords(land_cover_image, n_per_class_per_region):\n",
    "    sample_points = []\n",
    "    region = ee.Geometry.MultiPolygon(WORLD_POLYGONS)\n",
    "    # for class_value in range(1, 11):  # Adjust based on land cover classification\n",
    "    #     class_region = land_cover_image.eq(class_value).And(ee.Image.pixelArea().gt(0)).clip(region)\n",
    "    points = land_cover_image.stratifiedSample(\n",
    "        numPoints=n_per_class_per_region, \n",
    "        # classBand=land_cover_image.bandNames().get(0), \n",
    "        region=region, \n",
    "        scale=300,  # Adjust scale as needed\n",
    "        geometries=True\n",
    "    )\n",
    "    sample_points.extend(points.aggregate_array('.geo').getInfo())\n",
    "    return sample_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90551542dc7124f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "land_cover_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa279ec3acd6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2013-03-01'\n",
    "end_date = '2021-12-01'\n",
    "dates = pd.date_range(start=start_date, end=end_date, freq='MS').strftime('%Y-%m-%d').tolist()\n",
    "err_image = None\n",
    "from google.api_core import retry\n",
    "\n",
    "@retry.Retry(deadline=10 * 60)\n",
    "def process_and_export(point, dates):\n",
    "    global err_image\n",
    "    ee.Initialize(project='bigdata-ahhcash', opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "    lon, lat = point['coordinates']\n",
    "    point_geometry = ee.Geometry.Point(lon, lat)\n",
    "\n",
    "    time_series_data = []\n",
    "    for date in dates:\n",
    "        try:\n",
    "            # Landsat 8 processing\n",
    "            # dask.distributed.print((date, (lon, lat)))\n",
    "            landsat_collection = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
    "                .filterBounds(point_geometry) \\\n",
    "                .filterDate(date, ee.Date(date).advance(1, 'month'))\n",
    "            landsat_image = landsat_collection.median()\n",
    "            landsat_ndvi = landsat_image.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI')\n",
    "    \n",
    "            # MODIS processing (assuming MOD13Q1)\n",
    "            modis_collection = ee.ImageCollection('MODIS/006/MOD13Q1') \\\n",
    "                .filterBounds(point_geometry) \\\n",
    "                .filterDate(date, ee.Date(date).advance(1, 'month'))\n",
    "            modis_image = modis_collection.median()\n",
    "            modis_ndvi = modis_image.select('NDVI')\n",
    "    \n",
    "            combined_ndvi = ee.Image.cat([landsat_ndvi, modis_ndvi])\n",
    "    \n",
    "            def download_as_numpy(url):\n",
    "                response = requests.get(url)\n",
    "                # If we get \"429: Too Many Requests\" errors, it's safe to retry the request.\n",
    "                # The Retry library only works with `google.api_core` exceptions.\n",
    "                if response.status_code == 429:\n",
    "                    raise exceptions.TooManyRequests(response.text)\n",
    "                response.raise_for_status()  # Raise an error for bad status codes\n",
    "                return np.load(io.BytesIO(response.content))\n",
    "        \n",
    "            numpy_data = download_as_numpy(combined_ndvi.getDownloadURL(params={\n",
    "                'region': point_geometry.buffer(5000).bounds().getInfo()['coordinates'],\n",
    "                'scale': 1000,  # Adjust scale as needed\n",
    "                'format': 'NPY'\n",
    "            }))\n",
    "    \n",
    "            _ = combined_ndvi.bandNames().getInfo()\n",
    "            # Add date information to the NumPy array\n",
    "            # numpy_data = np.expand_dims(numpy_data, axis=0)  # Add a time dimension\n",
    "            # numpy_data = np.insert(numpy_data, 0, date, axis=0)  # Insert date at the beginning\n",
    "    \n",
    "            data_with_date = (date, numpy_data)\n",
    "            time_series_data.append(data_with_date)\n",
    "        except Exception as e:\n",
    "            dask.distributed.print(date)\n",
    "            continue\n",
    "\n",
    "    # Extract metadata from the first image\n",
    "    projection = None\n",
    "    try:\n",
    "        projection = combined_ndvi.projection().getInfo()\n",
    "        crs = projection['crs']\n",
    "        transform = projection['transform']\n",
    "        scale_x = transform[0]\n",
    "        scale_y = -transform[4]\n",
    "    \n",
    "        # Create a DataFrame with metadata\n",
    "        metadata_df = pd.DataFrame({\n",
    "            'longitude': [lon],\n",
    "            'latitude': [lat],\n",
    "            'crs': [crs],\n",
    "            'scale_x': [scale_x],\n",
    "            'scale_y': [scale_y]\n",
    "        })\n",
    "    \n",
    "        # Save time series data and metadata\n",
    "        time_series_array = np.concatenate(time_series_data, axis=0)  # Combine into a single array\n",
    "        np.savez_compressed(f'stratified_data/time_series_{lon}_{lat}.npz', data=time_series_array)\n",
    "        metadata_df.to_csv(f'stratified_data/metadata_{lon}_{lat}.csv', index=False)\n",
    "    except Exception as e:\n",
    "        dask.distributed.print(\"error occurred outside the for loop\")\n",
    "    \n",
    "\n",
    "# Generate stratified sample points\n",
    "sample_points = get_stratified_sample_coords(land_cover_image, 2) \n",
    "\n",
    "# Parallelize processing using Dask\n",
    "sample_points_bag = db.from_sequence(sample_points)\n",
    "results = sample_points_bag.map(process_and_export, dates)  # Pass dates to the function\n",
    "results.compute()\n",
    "\n",
    "print(\"Dask operations completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2d56d639e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = [-119.1548600183745, 53.9257517606046]\n",
    "lon, lat = coordinates\n",
    "ee.Initialize(project='bigdata-ahhcash', opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "start_date = '2013-03-01'\n",
    "end_date = '2021-12-01'\n",
    "dates = pd.date_range(start=start_date, end=end_date, freq='MS').strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "for date in dates:\n",
    "    try:\n",
    "        point_geometry = ee.Geometry.Point(lon, lat)\n",
    "        landsat_collection = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
    "            .filterBounds(point_geometry) \\\n",
    "            .filterDate(ee.Date(date), ee.Date(date).advance(1, 'month').advance(-1, 'day'))\n",
    "        landsat_image = landsat_collection.median()\n",
    "        landsat_ndvi = landsat_image.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI')\n",
    "        print(landsat_ndvi.bandNames().getInfo())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "923d4487-f246-47f7-bd22-81bc97dbdca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-01-01T00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(ee.Date('2013-12-01').advance(1, 'month').format().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe7ef4-11f6-4c60-9462-61d27e6428e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
