{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Type handler registry overriding type \"<class 'float'>\" collision on scalar\n",
      "WARNING:absl:Type handler registry overriding type \"<class 'bytes'>\" collision on scalar\n",
      "WARNING:absl:Type handler registry overriding type \"<class 'numpy.number'>\" collision on scalar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from typing import Tuple, Literal, Union\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ml_collections\n",
    "import numpy as np\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "import tensorflow as tf\n",
    "from flax.metrics import tensorboard\n",
    "# from flax.core import FrozenDict\n",
    "from flax.training import orbax_utils, train_state\n",
    "from modal import App, Image, Volume, gpu\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "NUM_CLASSES = 9\n",
    "NUM_INPUTS = 3\n",
    "KERNEL_SIZE = 5\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "app = App(\"flax-climate-forecast\")\n",
    "volume = Volume.from_name(\"climate-forecast\")\n",
    "img = Image.debian_slim().pip_install(\n",
    "    \"flax\",\n",
    "    \"numpy\",\n",
    "    \"tensorflow[and-cuda]\",\n",
    "    \"tensorboard\",\n",
    "    \"tqdm\",\n",
    "    \"ml-collections\",\n",
    "    \"tensorrt\",\n",
    ")\n",
    "\n",
    "img = img.run_commands(\n",
    "    [\n",
    "        \"pip install -U 'jax[cuda12_pip]' -f 'https://storage.googleapis.com/jax-releases/jax_cuda_releases.html'\",\n",
    "        \"python -m site\",\n",
    "        \"pip list | grep nvidia\",\n",
    "        \"export PATH=/usr/local/cuda-12/bin:$PATH\",\n",
    "        \"export LD_LIBRARY_PATH=/usr/local/cuda-12/lib64:/usr/local/lib/python3.11/site-packages/tensorrt_libs/:$LD_LIBRARY_PATH:\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_example(serialized: bytes) -> Tuple[jax.Array, jax.Array]:\n",
    "    \"\"\"Parses and reads a training example from bytes.\n",
    "\n",
    "    Args:\n",
    "        serialized: Serialized example bytes.\n",
    "\n",
    "    Returns: An (inputs, labels) pair of arrays.\n",
    "    \"\"\"\n",
    "    npz = np.load(serialized)\n",
    "    inputs = npz[\"inputs\"]\n",
    "    labels_landcover = npz[\"labels_landcover\"]\n",
    "    labels_lst = npz[\"labels_lst\"]\n",
    "\n",
    "    return (inputs, labels_landcover, labels_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_invalid_output_temperatures(temperatures, valid_range=(200, 330)):\n",
    "    \"\"\"Interpolate temperatures outside the valid range using Gaussian filtering.\"\"\"\n",
    "    invalid_mask = (temperatures < valid_range[0]) | (temperatures > valid_range[1])\n",
    "    temperatures_filtered = gaussian_filter(temperatures, sigma=1)\n",
    "    temperatures[invalid_mask] = temperatures_filtered[invalid_mask]\n",
    "    return temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_invalid_temperatures(data, valid_range=(200, 330), band_index=2):\n",
    "    \"\"\"Interpolate temperatures outside the valid range using Gaussian filtering.\"\"\"\n",
    "    errs = 0\n",
    "    for i in range(data.shape[0]):\n",
    "        invalid_mask = (data[i, :, :, band_index] < valid_range[0]) | (data[i, :, :, band_index] > valid_range[1])\n",
    "        if np.any(invalid_mask):  # Only apply filtering if there are any invalid values\n",
    "            errs += 1\n",
    "            valid_temperatures = gaussian_filter(data[i, :, :, band_index], sigma=1)\n",
    "            interpolated_values = np.where(invalid_mask, valid_temperatures, data[i, :, :, band_index])\n",
    "            data[i, :, :, band_index] = np.clip(interpolated_values, valid_range[0], valid_range[1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(\n",
    "    data_path: str, train_test_ratio: float\n",
    ") -> Tuple[Tuple[jax.Array, jax.Array], Tuple[jax.Array, jax.Array]]:\n",
    "    files = glob(os.path.join(data_path, \"*.npz\"))\n",
    "    # files = files[:2]\n",
    "    # Load data from npz files\n",
    "    inputs_list = []\n",
    "    lc_label_list = []\n",
    "    lst_label_list = []\n",
    "    for file in files:\n",
    "        with open(file, \"rb\") as f:\n",
    "            inputs, labels_landcover, labels_lst = read_example(f)\n",
    "            inputs = interpolate_invalid_temperatures(inputs)\n",
    "            labels_lst = interpolate_invalid_temperatures(labels_lst, band_index=0)\n",
    "            inputs_list.append(inputs)\n",
    "            lc_label_list.append(labels_landcover)\n",
    "            lst_label_list.append(labels_lst)\n",
    "\n",
    "    # Concatenate data\n",
    "    inputs = np.concatenate(inputs_list, axis=0)\n",
    "    labels_landcover = np.concatenate(lc_label_list, axis=0)\n",
    "    labels_lst = np.concatenate(lst_label_list, axis=0)\n",
    "    print(\n",
    "        f\"Inputs: {inputs.shape}, Labels Landcover: {labels_landcover.shape}, Labels LST: {labels_lst.shape}\"\n",
    "    )\n",
    "\n",
    "    train_size = int(inputs.shape[0] * train_test_ratio)\n",
    "    train_inputs, test_inputs = inputs[:train_size], inputs[train_size:]\n",
    "    train_labels_landcover, test_labels_landcover = (\n",
    "        labels_landcover[:train_size],\n",
    "        labels_landcover[train_size:],\n",
    "    )\n",
    "    train_labels_lst, test_labels_lst = labels_lst[:train_size], labels_lst[train_size:]\n",
    "\n",
    "    print(\n",
    "        f\"Training data: {train_inputs.shape}, Landcover: {train_labels_landcover.shape}, LST: {train_labels_lst.shape}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Testing data: {test_inputs.shape}, Landcover: {test_labels_landcover.shape}, LST: {test_labels_lst.shape}\"\n",
    "    )\n",
    "\n",
    "    return (train_inputs, train_labels_landcover, train_labels_lst), (\n",
    "        test_inputs,\n",
    "        test_labels_landcover,\n",
    "        test_labels_lst,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = read_dataset(\"../data/v2/climate_change/\", 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Fully Convolutional Network.\n",
    "class CNN_LandCover(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=32, kernel_size=(KERNEL_SIZE, KERNEL_SIZE))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.ConvTranspose(features=16, kernel_size=(KERNEL_SIZE, KERNEL_SIZE))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))  # Flatten\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=NUM_CLASSES)(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LST(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=64, kernel_size=(KERNEL_SIZE, KERNEL_SIZE))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.ConvTranspose(features=128, kernel_size=(KERNEL_SIZE, KERNEL_SIZE))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=1)(x)\n",
    "        x = nn.relu(x)  # No negative temperatures (since it is in Kelvin)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def apply_lc(state, images, lc):\n",
    "    \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({\"params\": params}, images)\n",
    "        one_hot = jax.nn.one_hot(lc, NUM_CLASSES)\n",
    "        loss = optax.losses.softmax_cross_entropy(\n",
    "            logits=logits, labels=one_hot\n",
    "        ).mean()  # Softmax Cross Entropy for Classification\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    accuracy_c = jnp.mean(jnp.argmax(logits, -1) == lc)\n",
    "    return grads, loss, accuracy_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def apply_lst(state, images, lst):\n",
    "    \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({\"params\": params}, images)\n",
    "        loss = optax.losses.squared_error(\n",
    "            predictions=logits, targets=lst\n",
    "        ).mean()  # MSE for Regression\n",
    "        return loss\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=False)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    return grads, loss, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, batch_size, rng, label: Literal[\"lc\", \"lst\"]):\n",
    "    \"\"\"Train for a single epoch.\"\"\"\n",
    "    train_ds_size = len(train_ds[0])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, len(train_ds[0]))\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_accuracy = []\n",
    "\n",
    "    for perm in perms:\n",
    "        batch_images = jnp.array(train_ds[0][perm, ...], dtype=jnp.float32)\n",
    "        batch_images = jax.nn.standardize(batch_images)\n",
    "\n",
    "        if label == \"lc\":\n",
    "            batch_labels = jnp.array(train_ds[1][perm, ...], dtype=jnp.uint8)\n",
    "            grads, loss, acc = apply_lc(state, batch_images, batch_labels)\n",
    "        else:\n",
    "            batch_labels = jnp.array(train_ds[2][perm, ...], dtype=jnp.float32)\n",
    "            grads, loss, acc = apply_lst(state, batch_images, batch_labels)\n",
    "        state = update_model(state, grads)\n",
    "        epoch_loss.append(loss)\n",
    "        if label == \"lc\":\n",
    "            epoch_accuracy.append(acc)\n",
    "    train_loss = np.mean(epoch_loss)\n",
    "    train_accuracy = None\n",
    "    if label == \"lc\":\n",
    "        train_accuracy = np.mean(epoch_accuracy)\n",
    "    return state, train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     tx \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madam(config\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_state\u001b[38;5;241m.\u001b[39mTrainState\u001b[38;5;241m.\u001b[39mcreate(apply_fn\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mapply, params\u001b[38;5;241m=\u001b[39mparams, tx\u001b[38;5;241m=\u001b[39mtx)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m     15\u001b[0m     image\u001b[38;5;241m=\u001b[39mimg,\n\u001b[1;32m     16\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m24\u001b[39m,\n\u001b[1;32m     17\u001b[0m     volumes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/vol\u001b[39m\u001b[38;5;124m\"\u001b[39m: volume},\n\u001b[1;32m     18\u001b[0m     gpu\u001b[38;5;241m=\u001b[39mgpu\u001b[38;5;241m.\u001b[39mA100(count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     19\u001b[0m     _allow_background_volume_commits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_evaluate\u001b[39m(\n\u001b[1;32m     22\u001b[0m     config: ml_collections\u001b[38;5;241m.\u001b[39mConfigDict,\n\u001b[1;32m     23\u001b[0m     data_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     24\u001b[0m     work_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     25\u001b[0m     ckpt_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     26\u001b[0m     label: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlst\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     27\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m train_state\u001b[38;5;241m.\u001b[39mTrainState:\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute model training and evaluation loop.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m      The train state (which includes the `.params`).\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'app' is not defined"
     ]
    }
   ],
   "source": [
    "def create_train_state(rng, config, label: Literal[\"lc\", \"lst\"]):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    if label == \"lc\":\n",
    "        model = CNN_LandCover()\n",
    "    elif label == \"lst\":\n",
    "        model = CNN_LST()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {label}\")\n",
    "    params = model.init(rng, jnp.ones([1, 128, 128, NUM_INPUTS]))[\"params\"]\n",
    "    tx = optax.adam(config.learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "\n",
    "@app.function(\n",
    "    image=img,\n",
    "    timeout=60 * 60 * 24,\n",
    "    volumes={\"/vol\": volume},\n",
    "    gpu=gpu.A100(count=1),\n",
    "    _allow_background_volume_commits=True,\n",
    ")\n",
    "def train_and_evaluate(\n",
    "    config: ml_collections.ConfigDict,\n",
    "    data_dir: str,\n",
    "    work_dir: str,\n",
    "    ckpt_dir: str,\n",
    "    label: Literal[\"lc\", \"lst\"],\n",
    ") -> train_state.TrainState:\n",
    "    \"\"\"Execute model training and evaluation loop.\n",
    "\n",
    "    Args:\n",
    "      config: Hyperparameter configuration for training and evaluation.\n",
    "      work_dir: Directory where the tensorboard summaries are written to.\n",
    "\n",
    "    Returns:\n",
    "      The train state (which includes the `.params`).\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    os.makedirs(work_dir, exist_ok=True)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    shutil.rmtree(ckpt_dir, ignore_errors=True)\n",
    "\n",
    "    ckpt_options = ocp.CheckpointManagerOptions(\n",
    "        max_to_keep=3,\n",
    "    )\n",
    "    ckpt_manager = ocp.CheckpointManager(\n",
    "        ocp.test_utils.erase_and_create_empty(ckpt_dir),\n",
    "        options=ckpt_options,\n",
    "    )\n",
    "\n",
    "    print(f\"JAX process: {jax.process_index()} / {jax.process_count()}\")\n",
    "    print(f\"JAX local devices: {jax.local_devices()}\")\n",
    "    train_ds, test_ds = read_dataset(data_dir, config.train_test_split)\n",
    "    rng = jax.random.key(0)\n",
    "\n",
    "    summary_writer = tensorboard.SummaryWriter(work_dir)\n",
    "    summary_writer.hparams(dict(config))\n",
    "\n",
    "    rng, init_rng = jax.random.split(rng)\n",
    "    state = create_train_state(init_rng, config, label)\n",
    "\n",
    "    test_images = jnp.array(test_ds[0], dtype=jnp.float32)\n",
    "    test_images = jax.nn.standardize(test_images)\n",
    "\n",
    "    if label == \"lc\":\n",
    "        test_labels = jnp.array(test_ds[1], dtype=jnp.uint8)\n",
    "    elif label == \"lst\":\n",
    "        test_labels = jnp.array(test_ds[2], dtype=jnp.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {label}\")\n",
    "\n",
    "    for epoch in tqdm(range(config.num_epochs)):\n",
    "        rng, input_rng = jax.random.split(rng)\n",
    "        state, train_loss, train_accuracy = train_epoch(\n",
    "            state, train_ds, config.batch_size, input_rng, label\n",
    "        )\n",
    "\n",
    "        if label == \"lc\":\n",
    "            _, test_loss, test_accuracy = apply_lc(state, test_images, test_labels)\n",
    "        elif label == \"lst\":\n",
    "            _, test_loss, test_accuracy = apply_lst(state, test_images, test_labels)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown label: {label}\")\n",
    "\n",
    "        print(f\"epoch:{epoch}, train_loss: {train_loss}, test_loss: {test_loss}\")\n",
    "        if label == \"lc\":\n",
    "            print(\n",
    "                f\"epoch:{epoch}, train_accuracy: {train_accuracy * 100}, test_accuracy: {test_accuracy * 100}\"\n",
    "            )\n",
    "            summary_writer.scalar(\"train_accuracy\", train_accuracy, epoch)\n",
    "            summary_writer.scalar(\"test_accuracy\", test_accuracy, epoch)\n",
    "\n",
    "        summary_writer.scalar(\"train_loss\", train_loss, epoch)\n",
    "        summary_writer.scalar(\"test_loss\", test_loss, epoch)\n",
    "\n",
    "        ckpt = {\"model\": state}\n",
    "        ckpt_manager.save(epoch, args=ocp.args.StandardSave(ckpt))\n",
    "        ckpt_manager.wait_until_finished()\n",
    "\n",
    "    summary_writer.flush()\n",
    "    volume.commit()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ml_collections.ConfigDict()\n",
    "\n",
    "config.learning_rate = 0.0002\n",
    "config.batch_size = 32\n",
    "config.num_epochs = 100\n",
    "config.train_test_split = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.abspath(\"../models/flax/lc\")\n",
    "# state = train_and_evaluate(\n",
    "#     config, \"../data/v2/climate_change\", \"../models/flax/logs\", ckpt_dir, \"lst\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with app.run(detach=True):\n",
    "    train_and_evaluate.remote(\n",
    "        config,\n",
    "        \"/vol/v2/data/\",\n",
    "        \"/vol/flax/lc/logs\",\n",
    "        \"/vol/flax/lc/checkpoints\",\n",
    "        \"lc\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "NOT_FOUND: Error opening \"cast\" driver: Error opening \"zarr\" driver: Error reading \"model.params.Dense_1.kernel/.zarray\" in OCDBT database at local file \"/Users/ro/nyu/big_data/project/models/flax/lc/100/default/\": Error reading Byte range [6185, 7239) of local file \"/Users/ro/nyu/big_data/project/models/flax/lc/100/default/d/bfec03604f22d578ff0b765ef3aa500e\" [source locations='tensorstore/kvstore/kvstore.cc:372\\ntensorstore/kvstore/kvstore.cc:372\\ntensorstore/driver/driver.cc:112\\ntensorstore/driver/driver.cc:112'] [tensorstore_spec='{\\\"context\\\":{\\\"cache_pool\\\":{},\\\"cache_pool#ocdbt\\\":{\\\"total_bytes_limit\\\":100000000},\\\"data_copy_concurrency\\\":{},\\\"file_io_concurrency\\\":{\\\"limit\\\":128},\\\"file_io_sync\\\":true,\\\"ocdbt_coordinator\\\":{}},\\\"driver\\\":\\\"zarr\\\",\\\"kvstore\\\":{\\\"base\\\":{\\\"driver\\\":\\\"file\\\",\\\"path\\\":\\\"/Users/ro/nyu/big_data/project/models/flax/lc/100/default/\\\"},\\\"cache_pool\\\":\\\"cache_pool#ocdbt\\\",\\\"driver\\\":\\\"ocdbt\\\",\\\"experimental_read_coalescing_interval\\\":\\\"1ms\\\",\\\"experimental_read_coalescing_merged_bytes\\\":500000000000,\\\"experimental_read_coalescing_threshold_bytes\\\":1000000,\\\"path\\\":\\\"model.params.Dense_1.kernel/\\\"},\\\"recheck_cached_data\\\":false,\\\"recheck_cached_metadata\\\":false}'] [tensorstore_spec[1]='{\\\"base\\\":{\\\"driver\\\":\\\"zarr\\\",\\\"kvstore\\\":{\\\"base\\\":{\\\"driver\\\":\\\"file\\\",\\\"path\\\":\\\"/Users/ro/nyu/big_data/project/models/flax/lc/100/default/\\\"},\\\"cache_pool\\\":\\\"cache_pool#ocdbt\\\",\\\"driver\\\":\\\"ocdbt\\\",\\\"experimental_read_coalescing_interval\\\":\\\"1ms\\\",\\\"experimental_read_coalescing_merged_bytes\\\":500000000000,\\\"experimental_read_coalescing_threshold_bytes\\\":1000000,\\\"path\\\":\\\"model.params.Dense_1.kernel/\\\"},\\\"recheck_cached_data\\\":false,\\\"recheck_cached_metadata\\\":false},\\\"context\\\":{\\\"cache_pool\\\":{},\\\"cache_pool#ocdbt\\\":{\\\"total_bytes_limit\\\":100000000},\\\"data_copy_concurrency\\\":{},\\\"file_io_concurrency\\\":{\\\"limit\\\":128},\\\"file_io_sync\\\":true,\\\"ocdbt_coordinator\\\":{}},\\\"driver\\\":\\\"cast\\\",\\\"dtype\\\":\\\"float32\\\"}']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m ckpt_options \u001b[38;5;241m=\u001b[39m ocp\u001b[38;5;241m.\u001b[39mCheckpointManagerOptions(\n\u001b[1;32m     19\u001b[0m     max_to_keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m ckpt_manager \u001b[38;5;241m=\u001b[39m ocp\u001b[38;5;241m.\u001b[39mCheckpointManager(\n\u001b[1;32m     22\u001b[0m     ckpt_dir,\n\u001b[1;32m     23\u001b[0m     options\u001b[38;5;241m=\u001b[39mckpt_options,\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mckpt_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mocp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStandardRestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpytree\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m model\n",
      "File \u001b[0;32m~/envs/bigdata.venv/lib/python3.11/site-packages/orbax/checkpoint/checkpoint_manager.py:1070\u001b[0m, in \u001b[0;36mCheckpointManager.restore\u001b[0;34m(self, step, items, restore_kwargs, directory, args)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     args \u001b[38;5;241m=\u001b[39m typing\u001b[38;5;241m.\u001b[39mcast(args_lib\u001b[38;5;241m.\u001b[39mComposite, args)\n\u001b[1;32m   1069\u001b[0m restore_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_read_step_directory(step, directory)\n\u001b[0;32m-> 1070\u001b[0m restored \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkpointer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrestore_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_item:\n\u001b[1;32m   1072\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m restored[DEFAULT_ITEM_NAME]\n",
      "File \u001b[0;32m~/envs/bigdata.venv/lib/python3.11/site-packages/orbax/checkpoint/async_checkpointer.py:341\u001b[0m, in \u001b[0;36mAsyncCheckpointer.restore\u001b[0;34m(self, directory, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See superclass documentation.\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_until_finished()\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/bigdata.venv/lib/python3.11/site-packages/orbax/checkpoint/checkpointer.py:169\u001b[0m, in \u001b[0;36mCheckpointer.restore\u001b[0;34m(self, directory, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRestoring item from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, directory)\n\u001b[1;32m    168\u001b[0m ckpt_args \u001b[38;5;241m=\u001b[39m construct_checkpoint_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handler, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 169\u001b[0m restored \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished restoring checkpoint from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, directory)\n\u001b[1;32m    171\u001b[0m utils\u001b[38;5;241m.\u001b[39msync_global_processes(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCheckpointer:restore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_active_processes)\n",
      "File \u001b[0;32m~/envs/bigdata.venv/lib/python3.11/site-packages/orbax/checkpoint/composite_checkpoint_handler.py:470\u001b[0m, in \u001b[0;36mCompositeCheckpointHandler.restore\u001b[0;34m(self, directory, args)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    469\u001b[0m   handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_set_handler(item_name, arg)\n\u001b[0;32m--> 470\u001b[0m   restored[item_name] \u001b[38;5;241m=\u001b[39m \u001b[43mhandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_item_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompositeResults(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrestored)\n",
      "File \u001b[0;32m~/envs/bigdata.venv/lib/python3.11/site-packages/orbax/checkpoint/standard_checkpoint_handler.py:185\u001b[0m, in \u001b[0;36mStandardCheckpointHandler.restore\u001b[0;34m(self, directory, item, args)\u001b[0m\n\u001b[1;32m    176\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    177\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`StandardCheckpointHandler` expects a target tree to be provided for\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    178\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m restore. Not doing so is generally UNSAFE unless you know the\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    179\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m present topology to be the same one as the checkpoint was saved\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m under.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m   )\n\u001b[1;32m    182\u001b[0m   restore_args \u001b[38;5;241m=\u001b[39m checkpoint_utils\u001b[38;5;241m.\u001b[39mconstruct_restore_args(\n\u001b[1;32m    183\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata(directory)\n\u001b[1;32m    184\u001b[0m   )\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpytree_checkpoint_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTreeRestoreArgs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestore_args\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/bigdata.venv/lib/python3.11/site-packages/orbax/checkpoint/pytree_checkpoint_handler.py:1154\u001b[0m, in \u001b[0;36mPyTreeCheckpointHandler.restore\u001b[0;34m(self, directory, item, restore_args, transforms, transforms_default_to_original, legacy_transform_fn, args)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;66;03m# If metadata file was missing in the checkpoint, we need to decide\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;66;03m# restore_type based on RestoreArgs.\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m structure \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(\n\u001b[1;32m   1151\u001b[0m     _maybe_set_default_restore_types, structure, checkpoint_restore_args\n\u001b[1;32m   1152\u001b[0m )\n\u001b[0;32m-> 1154\u001b[0m restored_item \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_deserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_restore_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m legacy_transform_fn:\n\u001b[1;32m   1159\u001b[0m   restored_item \u001b[38;5;241m=\u001b[39m _transform_checkpoint(\n\u001b[1;32m   1160\u001b[0m       item,\n\u001b[1;32m   1161\u001b[0m       restored_item,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1164\u001b[0m       transforms_default_to_original,\n\u001b[1;32m   1165\u001b[0m   )\n",
      "File \u001b[0;32m~/envs/bigdata.venv/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/envs/bigdata.venv/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.7/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.7/lib/python3.11/asyncio/tasks.py:279\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/bigdata.venv/lib/python3.11/site-packages/orbax/checkpoint/pytree_checkpoint_handler.py:979\u001b[0m, in \u001b[0;36mPyTreeCheckpointHandler._maybe_deserialize\u001b[0;34m(self, structure, param_infos, restore_args)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m request \u001b[38;5;129;01min\u001b[39;00m batch_requests:\n\u001b[1;32m    976\u001b[0m   deserialized_batches_ops\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    977\u001b[0m       request\u001b[38;5;241m.\u001b[39mhandler\u001b[38;5;241m.\u001b[39mdeserialize(request\u001b[38;5;241m.\u001b[39minfos, request\u001b[38;5;241m.\u001b[39margs)\n\u001b[1;32m    978\u001b[0m   )\n\u001b[0;32m--> 979\u001b[0m deserialized_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mdeserialized_batches_ops)\n\u001b[1;32m    981\u001b[0m flat_restored \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m request, deserialized \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_requests, deserialized_batches):\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.7/lib/python3.11/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.7/lib/python3.11/asyncio/tasks.py:279\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/bigdata.venv/lib/python3.11/site-packages/orbax/checkpoint/type_handlers.py:1529\u001b[0m, in \u001b[0;36mArrayHandler.deserialize\u001b[0;34m(self, infos, args)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, args)\n\u001b[1;32m   1518\u001b[0m   deserialize_ops \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1519\u001b[0m       serialization\u001b[38;5;241m.\u001b[39masync_deserialize(\n\u001b[1;32m   1520\u001b[0m           sharding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1527\u001b[0m       )\n\u001b[1;32m   1528\u001b[0m   ]\n\u001b[0;32m-> 1529\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mdeserialize_ops)\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mlevel_debug():\n\u001b[1;32m   1532\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m ret:\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.7/lib/python3.11/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.7/lib/python3.11/asyncio/tasks.py:279\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/bigdata.venv/lib/python3.11/site-packages/jax/experimental/array_serialization/serialization.py:317\u001b[0m, in \u001b[0;36masync_deserialize\u001b[0;34m(in_sharding, tensorstore_spec, global_shape, dtype, byte_limiter, context, assume_metadata)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masync_deserialize\u001b[39m(\n\u001b[1;32m    309\u001b[0m     in_sharding: sharding_impls\u001b[38;5;241m.\u001b[39mXLACompatibleSharding,\n\u001b[1;32m    310\u001b[0m     tensorstore_spec: ts\u001b[38;5;241m.\u001b[39mSpec \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m     assume_metadata: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    316\u001b[0m ):\n\u001b[0;32m--> 317\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m ts\u001b[38;5;241m.\u001b[39mopen(\n\u001b[1;32m    318\u001b[0m       tensorstore_spec,\n\u001b[1;32m    319\u001b[0m       \u001b[38;5;28mopen\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    320\u001b[0m       assume_metadata\u001b[38;5;241m=\u001b[39massume_metadata,\n\u001b[1;32m    321\u001b[0m       context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    322\u001b[0m   )\n\u001b[1;32m    323\u001b[0m   shape \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m global_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m global_shape\n\u001b[1;32m    324\u001b[0m   new_shard_shape \u001b[38;5;241m=\u001b[39m in_sharding\u001b[38;5;241m.\u001b[39mshard_shape(\u001b[38;5;28mtuple\u001b[39m(shape))\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.7/lib/python3.11/asyncio/futures.py:287\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.7/lib/python3.11/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.11.7/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[0;31mValueError\u001b[0m: NOT_FOUND: Error opening \"cast\" driver: Error opening \"zarr\" driver: Error reading \"model.params.Dense_1.kernel/.zarray\" in OCDBT database at local file \"/Users/ro/nyu/big_data/project/models/flax/lc/100/default/\": Error reading Byte range [6185, 7239) of local file \"/Users/ro/nyu/big_data/project/models/flax/lc/100/default/d/bfec03604f22d578ff0b765ef3aa500e\" [source locations='tensorstore/kvstore/kvstore.cc:372\\ntensorstore/kvstore/kvstore.cc:372\\ntensorstore/driver/driver.cc:112\\ntensorstore/driver/driver.cc:112'] [tensorstore_spec='{\\\"context\\\":{\\\"cache_pool\\\":{},\\\"cache_pool#ocdbt\\\":{\\\"total_bytes_limit\\\":100000000},\\\"data_copy_concurrency\\\":{},\\\"file_io_concurrency\\\":{\\\"limit\\\":128},\\\"file_io_sync\\\":true,\\\"ocdbt_coordinator\\\":{}},\\\"driver\\\":\\\"zarr\\\",\\\"kvstore\\\":{\\\"base\\\":{\\\"driver\\\":\\\"file\\\",\\\"path\\\":\\\"/Users/ro/nyu/big_data/project/models/flax/lc/100/default/\\\"},\\\"cache_pool\\\":\\\"cache_pool#ocdbt\\\",\\\"driver\\\":\\\"ocdbt\\\",\\\"experimental_read_coalescing_interval\\\":\\\"1ms\\\",\\\"experimental_read_coalescing_merged_bytes\\\":500000000000,\\\"experimental_read_coalescing_threshold_bytes\\\":1000000,\\\"path\\\":\\\"model.params.Dense_1.kernel/\\\"},\\\"recheck_cached_data\\\":false,\\\"recheck_cached_metadata\\\":false}'] [tensorstore_spec[1]='{\\\"base\\\":{\\\"driver\\\":\\\"zarr\\\",\\\"kvstore\\\":{\\\"base\\\":{\\\"driver\\\":\\\"file\\\",\\\"path\\\":\\\"/Users/ro/nyu/big_data/project/models/flax/lc/100/default/\\\"},\\\"cache_pool\\\":\\\"cache_pool#ocdbt\\\",\\\"driver\\\":\\\"ocdbt\\\",\\\"experimental_read_coalescing_interval\\\":\\\"1ms\\\",\\\"experimental_read_coalescing_merged_bytes\\\":500000000000,\\\"experimental_read_coalescing_threshold_bytes\\\":1000000,\\\"path\\\":\\\"model.params.Dense_1.kernel/\\\"},\\\"recheck_cached_data\\\":false,\\\"recheck_cached_metadata\\\":false},\\\"context\\\":{\\\"cache_pool\\\":{},\\\"cache_pool#ocdbt\\\":{\\\"total_bytes_limit\\\":100000000},\\\"data_copy_concurrency\\\":{},\\\"file_io_concurrency\\\":{\\\"limit\\\":128},\\\"file_io_sync\\\":true,\\\"ocdbt_coordinator\\\":{}},\\\"driver\\\":\\\"cast\\\",\\\"dtype\\\":\\\"float32\\\"}']"
     ]
    }
   ],
   "source": [
    "def create_train_state(rng, config, label: Literal[\"lc\", \"lst\"]):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    if label == \"lc\":\n",
    "        model = CNN_LandCover()\n",
    "    elif label == \"lst\":\n",
    "        model = CNN_LST()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {label}\")\n",
    "    params = model.init(rng, jnp.ones([1, 128, 128, NUM_INPUTS]))[\"params\"]\n",
    "    tx = optax.adam(config.learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "\n",
    "rng = jax.random.key(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "state = create_train_state(init_rng, config, \"lc\")\n",
    "pytree = {\"model\": state}\n",
    "ckpt_options = ocp.CheckpointManagerOptions(\n",
    "    max_to_keep=3,\n",
    ")\n",
    "ckpt_manager = ocp.CheckpointManager(\n",
    "    ckpt_dir,\n",
    "    options=ckpt_options,\n",
    ")\n",
    "model = ckpt_manager.restore(100, args=ocp.args.StandardRestore(pytree))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
