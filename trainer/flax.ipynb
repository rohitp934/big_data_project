{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Type handler registry overriding type \"<class 'float'>\" collision on scalar\n",
      "WARNING:absl:Type handler registry overriding type \"<class 'bytes'>\" collision on scalar\n",
      "WARNING:absl:Type handler registry overriding type \"<class 'numpy.number'>\" collision on scalar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from typing import Tuple, Literal\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ml_collections\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow as tf\n",
    "from flax.metrics import tensorboard\n",
    "from flax.training import checkpoints, train_state\n",
    "from modal import App, Image, Volume, gpu\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "NUM_CLASSES = 9\n",
    "NUM_INPUTS = 3\n",
    "KERNEL_SIZE = 5\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "app = App(\"flax-climate-forecast\")\n",
    "volume = Volume.from_name(\"climate-forecast\")\n",
    "img = Image.debian_slim().pip_install(\n",
    "    \"flax\",\n",
    "    \"numpy\",\n",
    "    \"tensorflow[and-cuda]\",\n",
    "    \"tensorboard\",\n",
    "    \"tqdm\",\n",
    "    \"ml-collections\",\n",
    "    \"tensorrt\",\n",
    ")\n",
    "\n",
    "img = img.run_commands(\n",
    "    [\n",
    "        \"pip install -U 'jax[cuda12_pip]' -f 'https://storage.googleapis.com/jax-releases/jax_cuda_releases.html'\",\n",
    "        \"python -m site\",\n",
    "        \"pip list | grep nvidia\",\n",
    "        \"export PATH=/usr/local/cuda-12/bin:$PATH\",\n",
    "        \"export LD_LIBRARY_PATH=/usr/local/cuda-12/lib64:/usr/local/lib/python3.11/site-packages/tensorrt_libs/:$LD_LIBRARY_PATH:\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_example(serialized: bytes) -> Tuple[jax.Array, jax.Array]:\n",
    "    \"\"\"Parses and reads a training example from bytes.\n",
    "\n",
    "    Args:\n",
    "        serialized: Serialized example bytes.\n",
    "\n",
    "    Returns: An (inputs, labels) pair of arrays.\n",
    "    \"\"\"\n",
    "    npz = np.load(serialized)\n",
    "    inputs = npz[\"inputs\"]\n",
    "    labels_landcover = npz[\"labels_landcover\"]\n",
    "    labels_lst = npz[\"labels_lst\"]\n",
    "\n",
    "    return (inputs, labels_landcover, labels_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_invalid_output_temperatures(temperatures, valid_range=(200, 330)):\n",
    "    \"\"\"Interpolate temperatures outside the valid range using Gaussian filtering.\"\"\"\n",
    "    invalid_mask = (temperatures < valid_range[0]) | (temperatures > valid_range[1])\n",
    "    temperatures_filtered = gaussian_filter(temperatures, sigma=1)\n",
    "    temperatures[invalid_mask] = temperatures_filtered[invalid_mask]\n",
    "    return temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_invalid_temperatures(data, valid_range=(200, 330), band_index=2):\n",
    "    \"\"\"Interpolate temperatures outside the valid range using Gaussian filtering.\"\"\"\n",
    "    errs = 0\n",
    "    for i in range(data.shape[0]):\n",
    "        invalid_mask = (data[i, :, :, band_index] < valid_range[0]) | (data[i, :, :, band_index] > valid_range[1])\n",
    "        if np.any(invalid_mask):  # Only apply filtering if there are any invalid values\n",
    "            errs += 1\n",
    "            valid_temperatures = gaussian_filter(data[i, :, :, band_index], sigma=1)\n",
    "            interpolated_values = np.where(invalid_mask, valid_temperatures, data[i, :, :, band_index])\n",
    "            data[i, :, :, band_index] = np.clip(interpolated_values, valid_range[0], valid_range[1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(\n",
    "    data_path: str, train_test_ratio: float\n",
    ") -> Tuple[Tuple[jax.Array, jax.Array], Tuple[jax.Array, jax.Array]]:\n",
    "    files = glob(os.path.join(data_path, \"*.npz\"))\n",
    "    # files = files[:2]\n",
    "    # Load data from npz files\n",
    "    inputs_list = []\n",
    "    lc_label_list = []\n",
    "    lst_label_list = []\n",
    "    for file in files:\n",
    "        with open(file, \"rb\") as f:\n",
    "            inputs, labels_landcover, labels_lst = read_example(f)\n",
    "            inputs = interpolate_invalid_temperatures(inputs)\n",
    "            labels_lst = interpolate_invalid_temperatures(labels_lst, band_index=0)\n",
    "            inputs_list.append(inputs)\n",
    "            lc_label_list.append(labels_landcover)\n",
    "            lst_label_list.append(labels_lst)\n",
    "\n",
    "    # Concatenate data\n",
    "    inputs = np.concatenate(inputs_list, axis=0)\n",
    "    labels_landcover = np.concatenate(lc_label_list, axis=0)\n",
    "    labels_lst = np.concatenate(lst_label_list, axis=0)\n",
    "    print(\n",
    "        f\"Inputs: {inputs.shape}, Labels Landcover: {labels_landcover.shape}, Labels LST: {labels_lst.shape}\"\n",
    "    )\n",
    "\n",
    "    train_size = int(inputs.shape[0] * train_test_ratio)\n",
    "    train_inputs, test_inputs = inputs[:train_size], inputs[train_size:]\n",
    "    train_labels_landcover, test_labels_landcover = (\n",
    "        labels_landcover[:train_size],\n",
    "        labels_landcover[train_size:],\n",
    "    )\n",
    "    train_labels_lst, test_labels_lst = labels_lst[:train_size], labels_lst[train_size:]\n",
    "\n",
    "    print(\n",
    "        f\"Training data: {train_inputs.shape}, Landcover: {train_labels_landcover.shape}, LST: {train_labels_lst.shape}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Testing data: {test_inputs.shape}, Landcover: {test_labels_landcover.shape}, LST: {test_labels_lst.shape}\"\n",
    "    )\n",
    "\n",
    "    return (train_inputs, train_labels_landcover, train_labels_lst), (\n",
    "        test_inputs,\n",
    "        test_labels_landcover,\n",
    "        test_labels_lst,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = read_dataset(\"../data/v2/climate_change/\", 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Fully Convolutional Network.\n",
    "class CNN_LandCover(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=32, kernel_size=(KERNEL_SIZE, KERNEL_SIZE))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.ConvTranspose(features=16, kernel_size=(KERNEL_SIZE, KERNEL_SIZE))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=NUM_CLASSES)(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LST(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=32, kernel_size=(KERNEL_SIZE, KERNEL_SIZE))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.ConvTranspose(features=16, kernel_size=(KERNEL_SIZE, KERNEL_SIZE))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=1)(x)\n",
    "        x = nn.relu(x)  # No negative temperatures (since it is in Kelvin)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def apply_lc(state, images, lc):\n",
    "    \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "    # print(f\"images shape: {images.shape}, lc shape: {lc.shape}\")\n",
    "    one_hot = jax.nn.one_hot(lc[:,:,:,-1], NUM_CLASSES)\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({\"params\": params}, images)\n",
    "        loss = optax.losses.softmax_cross_entropy(\n",
    "            logits=logits, labels=one_hot\n",
    "        ).mean()  # Softmax Cross Entropy for Classification\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    accuracy_c = jnp.mean(jnp.argmax(logits, -1) == jnp.argmax(one_hot, -1))\n",
    "\n",
    "    return grads, loss, accuracy_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def apply_lst(state, images, lst):\n",
    "    \"\"\"Computes gradients, loss, and accuracy for a single batch.\"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        \"\"\"Calculate loss based on parameters.\"\"\"\n",
    "        # Generate logits based on current parameters.\n",
    "        logits = state.apply_fn({\"params\": params}, images)\n",
    "        # Compute mean squared error loss.\n",
    "        loss = optax.losses.squared_error(predictions=logits, targets=lst).mean()\n",
    "        return loss, logits\n",
    "\n",
    "    # Compute gradients and loss, grads needs to be based on params directly influencing loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "\n",
    "    return grads, loss, None, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, batch_size, rng, label: Literal[\"lc\", \"lst\"]):\n",
    "    \"\"\"Train for a single epoch.\"\"\"\n",
    "    train_ds_size = len(train_ds[0])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, len(train_ds[0]))\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_accuracy = []\n",
    "\n",
    "    for perm in perms:\n",
    "        batch_images = jnp.array(train_ds[0][perm, ...], dtype=jnp.float32)\n",
    "        batch_images = jax.nn.standardize(batch_images)\n",
    "\n",
    "        if label == \"lc\":\n",
    "            batch_labels = jnp.array(train_ds[1][perm, ...], dtype=jnp.uint8)\n",
    "            # print(f\"Batch images shape: {batch_images.shape}, Batch labels shape: {batch_labels.shape}\")\n",
    "            grads, loss, acc, _ = apply_lc(state, batch_images, batch_labels)\n",
    "        else:\n",
    "            batch_labels = jnp.array(train_ds[2][perm, ...], dtype=jnp.float32)\n",
    "            grads, loss, acc, _ = apply_lst(state, batch_images, batch_labels)\n",
    "        state = update_model(state, grads)\n",
    "        epoch_loss.append(loss)\n",
    "        if label == \"lc\":\n",
    "            epoch_accuracy.append(acc)\n",
    "    train_loss = np.mean(epoch_loss)\n",
    "    train_accuracy = None\n",
    "    if label == \"lc\":\n",
    "        train_accuracy = np.mean(epoch_accuracy)\n",
    "    return state, train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, config, label: Literal[\"lc\", \"lst\"]):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    if label == \"lc\":\n",
    "        model = CNN_LandCover()\n",
    "    elif label == \"lst\":\n",
    "        model = CNN_LST()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {label}\")\n",
    "    params = model.init(\n",
    "        rng, jnp.ones([1, config.img_size, config.img_size, NUM_INPUTS])\n",
    "    )[\"params\"]\n",
    "    tx = optax.adam(config.learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "\n",
    "def save_predictions(epoch, images, labels, preds, save_dir, label_type):\n",
    "    epoch_dir = os.path.join(save_dir, f\"epoch_{epoch}/{label_type}\")\n",
    "    os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "    np.save(os.path.join(epoch_dir, \"images.npy\"), images)\n",
    "    np.save(os.path.join(epoch_dir, \"labels.npy\"), labels)\n",
    "    np.save(os.path.join(epoch_dir, \"preds.npy\"), preds)\n",
    "\n",
    "\n",
    "@app.function(\n",
    "    image=img,\n",
    "    timeout=60 * 60 * 24,\n",
    "    volumes={\"/vol\": volume},\n",
    "    gpu=gpu.A100(count=1),\n",
    "    _allow_background_volume_commits=True,\n",
    ")\n",
    "def train_and_evaluate(\n",
    "    config: ml_collections.ConfigDict,\n",
    "    data_dir: str,\n",
    "    work_dir: str,\n",
    "    ckpt_dir: str,\n",
    "    label: Literal[\"lc\", \"lst\"],\n",
    "    test_save_dir: str,\n",
    ") -> train_state.TrainState:\n",
    "    \"\"\"Execute model training and evaluation loop.\n",
    "\n",
    "    Args:\n",
    "      config: Hyperparameter configuration for training and evaluation.\n",
    "      work_dir: Directory where the tensorboard summaries are written to.\n",
    "\n",
    "    Returns:\n",
    "      The train state (which includes the `.params`).\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    os.makedirs(work_dir, exist_ok=True)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    shutil.rmtree(ckpt_dir, ignore_errors=True)\n",
    "\n",
    "    print(f\"JAX process: {jax.process_index()} / {jax.process_count()}\")\n",
    "    print(f\"JAX local devices: {jax.local_devices()}\")\n",
    "    train_ds, test_ds = read_dataset(data_dir, config.train_test_split)\n",
    "    rng = jax.random.key(0)\n",
    "\n",
    "    summary_writer = tensorboard.SummaryWriter(work_dir)\n",
    "    summary_writer.hparams(dict(config))\n",
    "\n",
    "    rng, init_rng = jax.random.split(rng)\n",
    "    state = create_train_state(init_rng, config, label)\n",
    "\n",
    "    test_images = jnp.array(test_ds[0], dtype=jnp.float32)\n",
    "    test_images = jax.nn.standardize(test_images)\n",
    "\n",
    "    if label == \"lc\":\n",
    "        test_labels = jnp.array(test_ds[1], dtype=jnp.uint8)\n",
    "    elif label == \"lst\":\n",
    "        test_labels = jnp.array(test_ds[2], dtype=jnp.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {label}\")\n",
    "\n",
    "    for epoch in tqdm(range(config.num_epochs)):\n",
    "        rng, input_rng = jax.random.split(rng)\n",
    "        state, train_loss, train_accuracy = train_epoch(\n",
    "            state, train_ds, config.batch_size, input_rng, label\n",
    "        )\n",
    "\n",
    "        if label == \"lc\":\n",
    "            _, test_loss, test_accuracy, logits = apply_lc(\n",
    "                state, test_images, test_labels\n",
    "            )\n",
    "        elif label == \"lst\":\n",
    "            _, test_loss, test_accuracy, logits = apply_lst(\n",
    "                state, test_images, test_labels\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown label: {label}\")\n",
    "\n",
    "        print(f\"epoch:{epoch}, train_loss: {train_loss}, test_loss: {test_loss}\")\n",
    "        if label == \"lc\":\n",
    "            print(\n",
    "                f\"epoch:{epoch}, train_accuracy: {train_accuracy * 100}, test_accuracy: {test_accuracy * 100}\"\n",
    "            )\n",
    "            summary_writer.scalar(\"train_accuracy\", train_accuracy, epoch)\n",
    "            summary_writer.scalar(\"test_accuracy\", test_accuracy, epoch)\n",
    "\n",
    "        summary_writer.scalar(\"train_loss\", train_loss, epoch)\n",
    "        summary_writer.scalar(\"test_loss\", test_loss, epoch)\n",
    "\n",
    "        checkpoints.save_checkpoint(ckpt_dir, state, epoch, prefix=\"\", keep=3)\n",
    "        if epoch % 10 == 0:\n",
    "            # Save test preds\n",
    "            save_predictions(\n",
    "                epoch, test_images, test_labels, logits, test_save_dir, label\n",
    "            )\n",
    "\n",
    "    summary_writer.flush()\n",
    "    volume.commit()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ml_collections.ConfigDict()\n",
    "\n",
    "config.learning_rate = 0.001\n",
    "config.batch_size = 16\n",
    "config.num_epochs = 250\n",
    "config.img_size = 128\n",
    "config.train_test_split = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Land Cover Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with app.run(detach=True):\n",
    "    train_and_evaluate.remote(\n",
    "        config,\n",
    "        \"/vol/v3/data/climate_change/\",\n",
    "        \"/vol/v3/flax/lc/logs\",\n",
    "        \"/vol/v3/flax/lc/checkpoints\",\n",
    "        \"lc\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Land Surface Temperature (LST) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with app.run(detach=True):\n",
    "    train_and_evaluate.remote(\n",
    "        config,\n",
    "        \"/vol/v3/data/climate_change/\",\n",
    "        \"/vol/v3/flax/lst/logs\",\n",
    "        \"/vol/v3/flax/lst/checkpoints\",\n",
    "        \"lst\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
