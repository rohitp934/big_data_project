{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Import Libraries ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import ee\n",
    "from google.api_core import exceptions, retry\n",
    "import google.auth\n",
    "import numpy as np\n",
    "from numpy.lib.recfunctions import structured_to_unstructured\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Constants and Earth Engine Initialization ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 10  # meters per pixel\n",
    "LAND_COVER_DATASET = \"GOOGLE/DYNAMICWORLD/V1\"  # Dynamic World Land Cover dataset\n",
    "LAND_COVER_BAND = \"Map\"  # Land cover classification band\n",
    "WORLD_POLYGONS = [\n",
    "    # Americas\n",
    "    [(-33.0, -7.0), (-55.0, 53.0), (-166.0, 65.0), (-68.0, -56.0)],\n",
    "    # Africa, Asia, Europe\n",
    "    [\n",
    "        (74.0, 71.0),\n",
    "        (166.0, 55.0),\n",
    "        (115.0, -11.0),\n",
    "        (74.0, -4.0),\n",
    "        (20.0, -38.0),\n",
    "        (-29.0, 25.0),\n",
    "    ],\n",
    "    # Australia\n",
    "    [(170.0, -47.0), (179.0, -37.0), (167.0, -12.0), (128.0, 17.0), (106.0, -29.0)],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Earth Engine Initialization ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"ee-rohitp934\"\n",
    "# Use cli to authenticate\n",
    "# !earthengine authenticate\n",
    "\n",
    "# Or use the following code to authenticate\n",
    "def initialize_ee():\n",
    "  ee.Authenticate()\n",
    "  ee.Initialize(project=project, opt_url=\"https://earthengine-highvolume.googleapis.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Data Retreival Functions ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modis_ndvi(date: datetime) -> ee.Image:\n",
    "    \"\"\"Gets MODIS NDVI data for a given date.\"\"\"\n",
    "    return (\n",
    "        ee.ImageCollection(\"MODIS/006/MOD13A2\")\n",
    "        .filterDate(date, date + timedelta(days=1))\n",
    "        .select(\"NDVI\")\n",
    "        .first()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landsat_image(date: datetime) -> ee.Image:\n",
    "    \"\"\"Gets a Landsat 8 image for the selected date.\"\"\"\n",
    "    return (\n",
    "        ee.ImageCollection(\"LANDSAT/LC08/C01/T1_SR\")\n",
    "        .filterDate(date, date + timedelta(days=1))\n",
    "        .filterBounds(ee.Geometry.Polygon(WORLD_POLYGONS))\n",
    "        .sort(\"CLOUD_COVER\")\n",
    "        .first()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landsat_ndvi(image: ee.Image) -> ee.Image:\n",
    "    \"\"\"Calculates NDVI from a Landsat 8 image.\"\"\"\n",
    "    return image.normalizedDifference([\"B5\", \"B4\"]).rename(\"NDVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landsat_lst(image: ee.Image) -> ee.Image:\n",
    "    \"\"\"\n",
    "    Calculates Land Surface Temperature from a Landsat 8 image.\n",
    "    This function is based on the formula in the following page https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C02_T1_L2\n",
    "    \"\"\"\n",
    "    return image.select(\"ST_B10\").multiply(0.00341802).add(149.0).rename(\"LST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_land_cover(date: datetime) -> ee.Image:\n",
    "    \"\"\"Gets a Land Cover image for the given date.\"\"\"\n",
    "    return (\n",
    "        ee.ImageCollection(LAND_COVER_DATASET)\n",
    "        .filterDate(date, date + timedelta(days=1))\n",
    "        .select(\"label\")\n",
    "        .first()\n",
    "        .rename(\"landcover\")\n",
    "        .unmask(0)  # fill missing values with 0 (water)\n",
    "        .byte()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Input and Label Image Composition ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_image(date: datetime) -> ee.Image:\n",
    "    \"\"\"Gets an Earth Engine image with all the inputs for the model.\"\"\"\n",
    "    # Get MODIS NDVI\n",
    "    modis_ndvi = get_modis_ndvi(date)\n",
    "\n",
    "    # Get Landsat data\n",
    "    landsat_image = get_landsat_image(date)\n",
    "    landsat_ndvi = get_landsat_ndvi(landsat_image)\n",
    "    landsat_lst = get_landsat_lst(landsat_image)\n",
    "\n",
    "    # Combine all input data\n",
    "    return ee.Image([modis_ndvi, landsat_ndvi, landsat_lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_image(year: int) -> ee.Image:\n",
    "    \"\"\"Gets a Land Cover image for the selected year and preprocesses it.\"\"\"\n",
    "    land_cover = get_land_cover(year)\n",
    "    # Add preprocessing steps if needed (e.g., remapping land cover classes)\n",
    "    return land_cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Get input and labels for a given latitude and longitude ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry.Retry(deadline=10 * 60)  # seconds\n",
    "def get_patch(\n",
    "    image: ee.Image, lonlat: tuple[float, float], patch_size: int, scale: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Fetches a patch of pixels from Earth Engine.\"\"\"\n",
    "    point = ee.Geometry.Point(lonlat)\n",
    "    url = image.getDownloadURL(\n",
    "        {\n",
    "            \"region\": point.buffer(scale * patch_size / 2, 1).bounds(1),\n",
    "            \"dimensions\": [patch_size, patch_size],\n",
    "            \"format\": \"NPY\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Retry on \"Too Many Requests\" errors\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 429:\n",
    "        raise exceptions.TooManyRequests(response.text)\n",
    "\n",
    "    # Raise other exceptions\n",
    "    response.raise_for_status()\n",
    "    return np.load(io.BytesIO(response.content), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_patch(\n",
    "    date: datetime, lonlat: tuple[float, float], patch_size: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Gets the inputs patch of pixels for the given point and date.\"\"\"\n",
    "    image = get_inputs_image(date)\n",
    "    patch = get_patch(image, lonlat, patch_size, SCALE)\n",
    "    return structured_to_unstructured(patch)\n",
    "\n",
    "\n",
    "def get_labels_patch(\n",
    "    date: datetime, lonlat: tuple[float, float], patch_size: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Gets the labels patch of pixels for the given point and year.\"\"\"\n",
    "    image = get_labels_image(date)\n",
    "    patch = get_patch(image, lonlat, patch_size, SCALE)\n",
    "    return structured_to_unstructured(patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Imports ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import dask\n",
    "import dask.bag as db\n",
    "from dask.distributed import Client\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Configs ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000\n",
    "PATCH_SIZE = 128\n",
    "PARTITION_SIZE = 10\n",
    "START_DATE = \"2015-07-01\"\n",
    "END_DATE = \"2021-12-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Sample Points ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points(date: datetime) -> tuple:\n",
    "    \"\"\"Samples points within the defined polygon for the given year.\"\"\"\n",
    "    land_cover = get_land_cover(date)\n",
    "    points = land_cover.stratifiedSample(\n",
    "        numPoints=1,\n",
    "        region=ee.Geometry.Polygon(WORLD_POLYGONS),\n",
    "        scale=SCALE,\n",
    "        geometries=True,\n",
    "    )\n",
    "    point = points.toList(points.size()).getInfo()[0]\n",
    "    return (date, point[\"geometry\"][\"coordinates\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Prepare Training Data ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_example(date: datetime, point: tuple) -> tuple:\n",
    "    \"\"\"Gets an (inputs, labels) training example for land cover change prediction.\"\"\"\n",
    "    inputs = get_inputs_patch(date, point, PATCH_SIZE)\n",
    "    # Get land cover for the next day\n",
    "    labels = get_labels_patch(date + timedelta(days=1), point, PATCH_SIZE)\n",
    "    return (inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.distributed\n",
    "\n",
    "\n",
    "def try_get_example(date: datetime, point: tuple) -> tuple | None:\n",
    "    \"\"\"Wrapper to handle errors during training data generation.\"\"\"\n",
    "    initialize_ee()\n",
    "    dask.distributed.print(f\"Generating training data for {date} at {point}\")\n",
    "    try:\n",
    "        return get_training_example(date, point)\n",
    "    except Exception as e:\n",
    "        dask.distributed.print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_date(start: datetime, end: datetime):\n",
    "    \"\"\"Generate a random datetime between `start` and `end`\"\"\"\n",
    "    return start + timedelta(\n",
    "        # Get a random amount of seconds between `start` and `end`\n",
    "        seconds=random.randint(0, int((end - start).total_seconds())),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Dask Workflow for Dataset Creation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_npz(data: list[tuple[np.ndarray, np.ndarray]], data_path: str) -> str:\n",
    "    \"\"\"Writes an (inputs, labels) set of data into a compressed NumPy file.\n",
    "\n",
    "    Args:\n",
    "        batch: Batch of (inputs, labels) pairs of NumPy arrays.\n",
    "        data_path: Directory path to save files to.\n",
    "\n",
    "    Returns: The filename of the data file.\n",
    "    \"\"\"\n",
    "    initialize_ee()\n",
    "    dask.distributed.print(f\"Writing {len(data)} data points to {data_path}\")\n",
    "    filename = os.path.join(data_path, f\"{uuid.uuid4()}.npz\")\n",
    "    with open(filename, \"xb\") as f:\n",
    "        inputs = [x for (x, _) in data]\n",
    "        labels = [y for (_, y) in data]\n",
    "        np.savez_compressed(f, inputs=inputs, labels=labels)\n",
    "    logging.info(filename)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data_path: str, samples: int = NUM_SAMPLES) -> None:\n",
    "    \"\"\"Runs the Dask workflow to generate the dataset.\"\"\"\n",
    "\n",
    "    # Generate dates from the start date to the end date\n",
    "    start_date = datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(END_DATE, \"%Y-%m-%d\")\n",
    "\n",
    "    random_dates = [random_date(start_date, end_date) for _ in range(samples)]\n",
    "\n",
    "    with Client() as client:  # Start a Dask client\n",
    "        # Authenticate and initialize Earth Engine.\n",
    "        initialize_ee()\n",
    "        print(client)\n",
    "\n",
    "        def wrapper(data, data_path):\n",
    "            return write_npz(data, data_path)\n",
    "\n",
    "        bag = db.from_sequence(random_dates, npartitions=PARTITION_SIZE).map(\n",
    "            sample_points\n",
    "        )\n",
    "        training_data = bag.map(try_get_example).filter(lambda x: x is not None)\n",
    "\n",
    "        training_data.map_partitions(wrapper, data_path=data_path).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Perform Dataset Creation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:64672' processes=4 threads=8, memory=16.00 GiB>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 23:49:53,330 - distributed.worker - WARNING - Compute Failed\n",
      "Key:       ('filter-lambda-sample_points-try_get_example-wrapper-b9d7c970f068fadae26323a23cc9a1c7', 9)\n",
      "Function:  execute_task\n",
      "args:      ((subgraph_callable-3e9823a451188dfaf9dda3b26adb2f91, (<class 'filter'>, <function run.<locals>.<lambda> at 0x10b939760>, (<function map_chunk at 0x10ba962a0>, <function try_get_example at 0x10b9bb920>, [(<function map_chunk at 0x10ba962a0>, <function sample_points at 0x10bce0860>, [[datetime.datetime(2017, 2, 8, 9, 10, 37), datetime.datetime(2021, 1, 1, 21, 1, 4), datetime.datetime(2021, 9, 4, 21, 56, 18), datetime.datetime(2020, 10, 16, 9, 50, 29), datetime.datetime(2018, 5, 1, 9, 1, 41), datetime.datetime(2015, 10, 29, 16, 45, 31), datetime.datetime(2020, 3, 22, 1, 40, 39), datetime.datetime(2018, 2, 17, 2, 40, 16), datetime.datetime(2020, 11, 20, 9, 36, 25), datetime.datetime(2015, 11, 27, 0, 17, 39), datetime.datetime(2017, 7, 9, 15, 55, 9), datetime.datetime(2020, 8, 19, 15, 7, 54), datetime.datetime(2018, 3, 27, 1, 19, 9), datetime.datetime(2021, 8, 25, 3, 11, 58), datetime.datetime(2019, 8, 30, 4, 9, 12), datetime.datetime(2016, 7, 15, 23, 28, 47), datetime.datetime(2021, 2, 22\n",
      "kwargs:    {}\n",
      "Exception: 'TypeError(\"object of type \\'filter\\' has no len()\")'\n",
      "\n",
      "2024-04-22 23:49:53,346 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('filter-lambda-sample_points-try_get_example-wrapper-b9d7c970f068fadae26323a23cc9a1c7', 5))\" coro=<Worker.execute() done, defined at /Users/ro/envs/bigdata.venv/lib/python3.11/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-04-22 23:49:53,346 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('filter-lambda-sample_points-try_get_example-wrapper-b9d7c970f068fadae26323a23cc9a1c7', 7))\" coro=<Worker.execute() done, defined at /Users/ro/envs/bigdata.venv/lib/python3.11/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-04-22 23:49:53,346 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('filter-lambda-sample_points-try_get_example-wrapper-b9d7c970f068fadae26323a23cc9a1c7', 1))\" coro=<Worker.execute() done, defined at /Users/ro/envs/bigdata.venv/lib/python3.11/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-04-22 23:49:53,346 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('filter-lambda-sample_points-try_get_example-wrapper-b9d7c970f068fadae26323a23cc9a1c7', 3))\" coro=<Worker.execute() done, defined at /Users/ro/envs/bigdata.venv/lib/python3.11/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-04-22 23:49:53,346 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('filter-lambda-sample_points-try_get_example-wrapper-b9d7c970f068fadae26323a23cc9a1c7', 6))\" coro=<Worker.execute() done, defined at /Users/ro/envs/bigdata.venv/lib/python3.11/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-04-22 23:49:53,346 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('filter-lambda-sample_points-try_get_example-wrapper-b9d7c970f068fadae26323a23cc9a1c7', 2))\" coro=<Worker.execute() done, defined at /Users/ro/envs/bigdata.venv/lib/python3.11/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-04-22 23:49:53,347 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('filter-lambda-sample_points-try_get_example-wrapper-b9d7c970f068fadae26323a23cc9a1c7', 8))\" coro=<Worker.execute() done, defined at /Users/ro/envs/bigdata.venv/lib/python3.11/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-04-22 23:49:53,347 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('filter-lambda-sample_points-try_get_example-wrapper-b9d7c970f068fadae26323a23cc9a1c7', 4))\" coro=<Worker.execute() done, defined at /Users/ro/envs/bigdata.venv/lib/python3.11/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'filter' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m logging\u001b[38;5;241m.\u001b[39mgetLogger()\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/climate_change/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 23\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(data_path, samples)\u001b[0m\n\u001b[1;32m     18\u001b[0m bag \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mfrom_sequence(random_dates, npartitions\u001b[38;5;241m=\u001b[39mPARTITION_SIZE)\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     19\u001b[0m     sample_points\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m training_data \u001b[38;5;241m=\u001b[39m bag\u001b[38;5;241m.\u001b[39mmap(try_get_example)\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtraining_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/bigdata.venv/lib/python3.11/site-packages/dask/base.py:375\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/envs/bigdata.venv/lib/python3.11/site-packages/dask/base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "Cell \u001b[0;32mIn[53], line 16\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(data, data_path):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m write_npz(data, data_path)\n",
      "Cell \u001b[0;32mIn[48], line 11\u001b[0m, in \u001b[0;36mwrite_npz\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Writes an (inputs, labels) set of data into a compressed NumPy file.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mReturns: The filename of the data file.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m initialize_ee()\n\u001b[0;32m---> 11\u001b[0m dask\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data points to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'filter' has no len()"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "run(\"data/climate_change/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
